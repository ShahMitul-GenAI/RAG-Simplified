{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abbe95a-9384-427b-9921-f81c4bb05f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os.path\n",
    "import pickle\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import Tool\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema.document import Document\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain.agents.initialize import initialize_agent\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.utilities import WikipediaAPIWrapper \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a485ef8d-26a9-49b6-b947-371edaf7ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading API keys from env\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb9e76a-ca4a-4b7d-9412-160816ae149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model and defining embedding\n",
    "llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo')\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa796f0-522d-42cb-9e63-3a1ef35edc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing inputs from UI \n",
    "fslk = open(\"./notifications/option.txt\", \"r\")\n",
    "selection = fslk.read()\n",
    "fslk.close()\n",
    "\n",
    "# determining prompts as per search selection option \n",
    "if selection == \"Wikipedia\":\n",
    "    while not os.path.exists(\"./notifications/wiki_query.txt\"):\n",
    "        time.sleep(5)\n",
    "    if os.path.exists(\"./notifications/wiki_query.txt\"):\n",
    "        f1 = open(\"./notifications/wiki_query.txt\", 'r')\n",
    "        query = f1.read()\n",
    "        f1.close()\n",
    "        print(f\"wiki prompt after selection is {pquery}\")\n",
    "elif selection == \"Research Paper\":\n",
    "     while not os.path.exists(\"./notifications/doc_query.txt\"):\n",
    "        time.sleep(5)\n",
    "    if os.path.exists(\"./notifications/doc_query.txt\"):\n",
    "        f2 = open(\"./notifications/doc_query.txt\", 'r')\n",
    "        query = f2.read()\n",
    "        f2.close()\n",
    "        print(f\"Docu prompt after selection is {query}\")\n",
    "else:\n",
    "    while not os.path.exists(\"./notifications/cmd_qr2.txt.txt\"):\n",
    "        time.sleep(5)\n",
    "    if os.path(\"./notifications/cmd_qr2.txt\"):\n",
    "        f3 = open(\"./notifications/cmd_qr1.txt\", 'r')\n",
    "        query1 = f3.read()\n",
    "        f4 = open(\"./notifications/cmd_qr2.txt\", 'r')\n",
    "        query2 = f4.read()\n",
    "        f3.close()\n",
    "        f4.close()\n",
    "        print(f\"wiki3 prompt after selection is {query1}\")\n",
    "        print(f\"docu3 prompt after selection is {query2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ded0a-b50d-4b35-803c-2d3e65b24339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up info retreival from Wikipedia pages (1st knowledge source)\n",
    "if selection == \"Wikipedia\":\n",
    "    print(f\"wiki prompt insider wrapper is {query}\")\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    wiki_output = wikipedia.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356c153-042e-4f5e-83ea-5994e93e9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fragmegting the document content to fit in the number of token limitations\n",
    "if selection == \"Wikipedia\":\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
    "    new_docs = [Document(page_content=sent) for sent in wiki_output.split('\\n')]\n",
    "\n",
    "# splitted_output = text_splitter.split_documents(new_doc)\n",
    "    data_set = DocArrayInMemorySearch.from_documents(new_docs, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a76e54c-0e19-4ea8-9e16-e5b06219e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreiving the llm response using user query\n",
    "if selection == \"Wikipedia\":\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm =llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever = data_set.as_retriever(),\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a363a-489e-4e09-951d-47ba280d8951",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selection.strip() == \"Wikipedia\":\n",
    "    wiki_out = qa.invoke(query)\n",
    "    with open(\"./notifications/wiki_out.txt\", 'w') as fwk:\n",
    "        fwk.write(str(wiki_out)['result'])\n",
    "        pickle.dumps(\"wiki_out.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3528185-79d6-4eb5-bae4-e9f8e5e34e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading research paper from web source (2nd knoledge source)\n",
    "\n",
    "if selection == \"Research Paper\":\n",
    "    loader = PyPDFLoader(\"./2312.10997v5.pdf\")\n",
    "    docs = loader.load()\n",
    "\n",
    "# fragmegting the document content to fit in the number of token limitations\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# load the \n",
    "    data_set = DocArrayInMemorySearch.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa09c47-bb9b-41da-946d-a5cc8f722e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreiving the llm response using user query\n",
    "\n",
    "if selection == \"Research Paper\":\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm =llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever = data_set.as_retriever(),\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d711cc-89cc-410d-848c-799e0cefb58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get query from U/I now\n",
    "if selection == \"Research Paper\":\n",
    "    result = qa.invoke(query)\n",
    "    with open(\"./notifications/doc_out.txt\", 'w') as frp:\n",
    "        frp.write(str(resul['result']))\n",
    "    pickle.dumps(\"doc_out.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d2d9fb-481d-433f-bb23-a384c2f21077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining two RAG knoledge sources together for better performance\n",
    "\n",
    "if selection == \"Both\":\n",
    "    wiki_tool = Tool(\n",
    "        name = \"wikipedia\",\n",
    "        func = wikipedia.run,\n",
    "        description = \"A useful tool to search internet for the requested information\",\n",
    "    )\n",
    "    \n",
    "    docsearch_tool = Tool(\n",
    "        name = \"docsearch\",\n",
    "        func = qa.run,\n",
    "        description = \"A tool to search information from the pool of documents\",\n",
    "    )\n",
    "    \n",
    "    agent= initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose = False,\n",
    "        handle_pasring_errors = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c8caa-70a2-44f3-ab7b-6324c93ed3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# executing the agent for both knowledgebase options\n",
    "\n",
    "if selection == \"Both\":\n",
    "    result1 = agent.invoke(str(query1))\n",
    "    result2 = agent.invoke(str(query2))\n",
    "    \n",
    "    with open(\"./notifications/1.txt\", 'w') as f1:\n",
    "        f1.write(str(resul1['output']))\n",
    "        pickle.dumps(\"1.txt\")\n",
    "    with open(\"./notifications/2.txt\", 'w') as f2:\n",
    "        f2.write(str(result2['output']))\n",
    "        pickle.dumps(\"2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60737d9-c89a-46b9-ac0e-3b634df8d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting program ending indicator\n",
    "ffnl = open(\"./notifications/PROG EXIT.txt\", \"rb\")\n",
    "pickle.dumps(123, ffnl)\n",
    "ffnl.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
